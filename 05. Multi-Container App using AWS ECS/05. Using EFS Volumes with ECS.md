# **Using EFS Volumes with ECS (Persistent Storage for MongoDB)**

--> Explaining how to **persist data in AWS ECS** using **Amazon EFS** (Elastic File System) so that your MongoDB container does **not lose all data** whenever a task is stopped or redeployed.

---

# üî• **1. The Problem: Data Loss on Every Deployment**

Your MongoDB runs **inside a container**.
When ECS performs a **rolling deployment**, your old task stops ‚Üí the MongoDB container shuts down ‚Üí **all data stored inside the container disappears**.

This is expected:

* Local containers lose data unless you use **volumes**
* ECS containers behave exactly the same

So after a redeploy, your `/goals` API returns an **empty database**.

---

# üü¶ **2. The Solution: Use an EFS Volume with Fargate**

AWS Fargate is serverless ‚Äî you can't attach local disks ‚Äî
so the correct solution is **Amazon EFS**, a network file system.

EFS advantages:

‚úî Persistent
‚úî Survives task restarts & redeploys
‚úî Multiple containers can mount it (with limitations, explained later)

EFS serves as a **durable storage backend** for MongoDB‚Äôs `/data/db` directory.

---

# ‚öôÔ∏è **3. Steps to Enable EFS for the MongoDB Container**

### **A. Create a new Task Definition Revision**

Go to ECS ‚Üí Task Definitions ‚Üí create new revision.

You will add:

* A new **Volume**
* A mount point for MongoDB

---

### **B. Create an EFS File System**

Go to **Amazon EFS console ‚Üí Create file system**
Choose:

* Name: *any* (e.g., "db-storage")
* VPC: choose the same VPC used by ECS

Click **Customize** ‚Üí important step!

#### Under **Network Access**:

Add Mount Targets ‚Üí choose the VPC **subnets** (same ones as ECS).

---

### **C. Create a new Security Group for EFS**

Go to EC2 ‚Üí Security Groups ‚Üí Create new:

* Name: **efs-sg**
* VPC: same as ECS

Add **inbound rule**:

* Type: **NFS**
* Port: 2049
* Source: **your ECS tasks' security group (goals-sg)**

This allows the ECS containers to talk to EFS.

Attach this EFS security group to the new file system.

---

### **D. Back in ECS Task Definition: Add the EFS Volume**

* Type: **EFS**
* Choose the file system you just created
* No access point needed (using root FS directly)

---

### **E. Define the mount point for MongoDB**

In the MongoDB container settings:

Storage ‚Üí Mount Points:

* Source volume: the EFS volume
* Container path: **/data/db**

This matches MongoDB's internal persistent data location.

---

### **F. Deploy the new revision**

Update service ‚Üí **Force new deployment**

‚ö† Choose **Platform version 1.4** because:

* "Latest" doesn‚Äôt actually support EFS at this time

---

# üéâ **4. After Deployment: Data Persistence Works**

Now your MongoDB data is stored in EFS ‚Üí survives task shutdowns.

Verify:

1. Add a goal
2. Force a redeploy
3. Once new task is running, GET `/goals`

Your data is still there. ‚úî

---

# ‚ö†Ô∏è **5. The Problem: Rolling Deployments + MongoDB Lock File**

During rolling deployments:

* ECS runs **two tasks at the same time**
* Both MongoDB containers mount the **same EFS path**
* Both try to access the same database files ‚Üí **LOCK FILE ERROR**

Error seen in logs:

```
Unable to lock the lock file: Resource temporarily unavailable
Another mongod instance is already running
```

MongoDB **cannot be shared between two running containers**, even with EFS.
Only one instance can access the database files at a time.

### This is a *MongoDB architectural limitation*, not EFS's fault.

---

# üõ† **6. Workaround (Temporary)**

To continue:

* Manually **stop the old task** so the new task has exclusive access
* Then MongoDB starts successfully

But this is not a proper production solution.

---

# üîÑ **7. Why This Setup Will Be Replaced**

Because:

* MongoDB cannot safely run on a shared file system like EFS
* Rolling deployments break it
* It is not how MongoDB is meant to be deployed in production

We will later replace MongoDB with a **managed database service** (RDS / Atlas / DynamoDB depending on the stack).

